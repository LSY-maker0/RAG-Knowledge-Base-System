"""
retrieval - æ£€ç´¢å™¨ï¼Œæ£€ç´¢å‡ºå¯¹åº”é—®é¢˜çš„å—

Author: lsy
Date: 2026/1/7
"""
import os
import time
from typing import List,Dict
from pathlib import Path
import json
import faiss
import dashscope
import numpy as np
import glob
from rank_bm25 import BM25Okapi
import jieba
from src.reranking import LLMReranker

class BM25Retriever:
    def __init__(self, metadata_path: Path):
        """
        åˆå§‹åŒ– BM25 æ£€ç´¢å™¨
        :param metadata_path: å­˜æ”¾åˆ†å— json æ–‡ä»¶çš„ç›®å½•è·¯å¾„
        """
        self.documents = []
        self.corpus_tokens = []
        self.bm25 = None

        print(f"[BM25] æ­£åœ¨ä» {metadata_path} åŠ è½½æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•...")
        self._load_and_index(metadata_path)

    def _load_and_index(self, metadata_path:Path):
        with open(metadata_path,'r',encoding='utf-8') as f:
            chunks = json.load(f)
            for chunk in chunks:
                self.documents.append(chunk)

        # 2. åˆ†è¯ - ä½¿ç”¨jiebaå°†ä¸­æ–‡æ–‡æœ¬åˆ‡æˆè¯è¯­åˆ—è¡¨ï¼ˆæ„å»ºç´¢å¼•çš„å…³é”®ï¼‰
        self.corpus_tokens = [list(jieba.cut(doc['text'])) for doc in self.documents]

        # 3. åˆå§‹åŒ– BM25 æ¨¡å‹
        self.bm25 = BM25Okapi(self.corpus_tokens)

    @staticmethod
    def normalize_scores(scores):
        """Min-Max å½’ä¸€åŒ–åˆ° [0, 1]"""
        scores = np.array(scores)
        min_score = scores.min()
        max_score = scores.max()

        # é¿å…é™¤ä»¥ 0
        if max_score == min_score:
            return np.zeros_like(scores)

        return (scores - min_score) / (max_score - min_score)

    def retrieve(self, question:str,top_n:int=20):
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        question_tokens = list(jieba.cut(question)) # é—®é¢˜åˆ†è¯
        raw_scores = self.bm25.get_scores(question_tokens) # è·å–æ–‡æ¡£å¾—åˆ†ï¼ˆè¿”å›çš„æ˜¯æ–‡æ¡£åœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•ï¼‰
        normalized_scores = self.normalize_scores(raw_scores) # å½’ä¸€åŒ–
        top_n_indices = normalized_scores.argsort()[-top_n:][::-1] # å€’åºå–å‰kä¸ª
        # ç»„è£…ç»“æœ
        results = []
        for index in top_n_indices:
            doc = self.documents[index].copy()
            # å°†BM25åˆ†æ•°æ·»åŠ åˆ°æ–‡æ¡£ä¸­ï¼ˆåˆ†æ•°è¶Šå¤§è¶Šå¥½ï¼‰
            doc['bm25_score'] = float(normalized_scores[index])
            results.append(doc)
        return results

class VectorRetriever:
    def __init__(self,vector_index_path:Path, metadata_path:Path,embedding_provider:str="dashscope"):
        """
        :param vector_index_path: FAISSå‘é‡ç´¢å¼•æ–‡ä»¶è·¯å¾„
        :param metadata_path: æ–‡æ¡£å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.vector_index_path=vector_index_path
        self.metadata_path=metadata_path
        self.embedding_provider=embedding_provider
        self._set_up_llm() # è®¾ç½®å¤§æ¨¡å‹æä¾›å•†

        # å®šä¹‰å®ä¾‹å˜é‡ä½†ä¸èµ‹å€¼ï¼Œç”¨äºåç»­ç¼“å­˜
        self._index = None
        self._metadata_list = None
        self.load()

    def load(self):
        """æ˜¾å¼åŠ è½½èµ„æºï¼Œä¹Ÿå¯ä»¥åœ¨é¦–æ¬¡æœç´¢æ—¶è‡ªåŠ¨è§¦å‘"""
        if self._index is None:
            self._load_index()
        if self._metadata_list is None:
            self._load_metadata()
        return self

    def _load_index(self):
        """åŠ è½½ FAISS ç´¢å¼•"""
        if not self.vector_index_path.exists():
            raise FileNotFoundError(f"å‘é‡ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {self.vector_index_path}")

        # faiss.read_index æ˜¯è¯»å–ç£ç›˜ä¸Šé¢„è®­ç»ƒå¥½çš„ç´¢å¼•çš„æ ‡å‡†æ–¹æ³•
        self._index = faiss.read_index(str(self.vector_index_path))

    def _load_metadata(self):
        """
        åŠ è½½å…ƒæ•°æ®
        å‡è®¾å…ƒæ•°æ®æ˜¯ç”¨ pickle å­˜å‚¨çš„ List[Dict] æˆ– DataFrame
        """
        if not self.metadata_path.exists():
            raise FileNotFoundError(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.metadata_path}")

        with open(self.metadata_path, 'rb') as f:
            self._metadata_list = json.load(f)

    def _set_up_llm(self):
        if self.embedding_provider=="dashscope":
            dashscope.api_key=os.getenv('DASHSCOPE_API_KEY')

    def _get_embedding(self,text:str):
        resp = dashscope.TextEmbedding.call(
            model='text-embedding-v1',
            input=[text],
        )
        embedding = resp.output['embeddings'][0]['embedding']  # List[float]
        vec = np.array(embedding, dtype='float32')
        # L2 å½’ä¸€åŒ–
        # è¿™ä¸€æ­¥è®©å‘é‡é•¿åº¦å˜ä¸º 1ï¼Œä»¥ä¾¿ä¸åº“é‡ŒåŒæ ·å½’ä¸€åŒ–çš„å‘é‡è¿›è¡Œä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
        norm = np.linalg.norm(vec)
        if norm > 0:
            vec = vec / norm
        return vec

    def get_relevant_chunks(self, question:str, top_n:int = 20) -> List[Dict]:
        # æ£€ç´¢å‡ºä¸é—®é¢˜ç›¸å…³çš„å—ï¼Œè¿”å›å…¨éƒ¨å—

        # è·å–queryçš„embeddingï¼Œæ”¯æŒdashscope
        embedding_question = self._get_embedding(question)
        # print(embedding_question,len(embedding_question),f'{question}çš„å‘é‡è¡¨è¡¨ç¤º')
        embedding_array = embedding_question.reshape(1, -1) # å˜ä¸ºäºŒç»´
        k = min(top_n, self._index.ntotal)
        # print(self._metadata_list,'å…ƒæ•°æ®')
        distances, indices = self._index.search(x=embedding_array,k=k)

        retrieval_results = []
        # print('distances:',distances)
        # print('indices:',indices)
        for distance, index in zip(distances[0], indices[0]):
            distance = float(distance)
            chunk=self._metadata_list[index]

            result = {
                "vector_score": distance,
                "page_range": chunk["page_range"],
                "file_origin": chunk["file_origin"],
                "text": chunk["text"],
            }
            retrieval_results.append(result)
        return retrieval_results

class HybridRetriever:
    def __init__(self,vector_index_path:Path, metadata_path:Path):
        self.vector_retriever = VectorRetriever(vector_index_path,metadata_path)
        self.bm25_retriever = BM25Retriever(metadata_path)
        self.reranker=LLMReranker()

    @staticmethod
    def _merge_hybrid_results(vector_results, bm25_results, x=0.6):
        """
        èåˆå‘é‡æ£€ç´¢å’ŒBM25æ£€ç´¢çš„ç»“æœ
        :param vector_results: å‘é‡æ£€ç´¢çš„ç»“æœ
        :param bm25_results: BM25æ£€ç´¢çš„ç»“æœ
        :param x: å‘é‡å æ¯”çš„æƒé‡
        :return: èåˆåçš„ç»“æœ
        """

        bm25_by_id = {}
        for res in bm25_results:
            chunk_id = res['text'][:50]
            bm25_by_id[chunk_id] = res

        # å»ºç«‹æ˜ å°„ï¼ˆç”¨textå†…å®¹ä½œä¸ºidè¿›è¡Œå»é‡å’Œå åŠ ï¼‰
        merged_map = {}

        for i, res in enumerate(vector_results):
            chunk_id = res['text'][:50]
            vector_score = float(res.get('vector_score', 0.0))

            bm25_res = bm25_by_id.get(chunk_id)
            bm25_score = float(bm25_res.get('bm25_score')) if bm25_res and bm25_res.get(
                'bm25_score') is not None else 0.0

            # åŸºäºå‘é‡ç»“æœä¸ºä¸»æ„é€ æ¡ç›®
            merged_item = dict(res)
            if bm25_res is not None and bm25_res.get('bm25_score') is not None:
                merged_item['bm25_score'] = bm25_res['bm25_score']

            final_score = x * vector_score + (1-x) * bm25_score
            merged_item['final_score'] = final_score

            merged_map[chunk_id] = merged_item

        final_list = [item for item in merged_map.values()]
        # æŠŠæ€»åˆ†å†™å›åˆ°å­—å…¸é‡Œ
        for i, item in enumerate(merged_map.values()):
            final_list[i]['final_score'] = item['final_score']
        final_list.sort(key=lambda x: x['final_score'], reverse=True)
        return final_list

    def __format_retrieval_results(self, retrieval_results) -> str:
        """å°†æ£€ç´¢ç»“æœè½¬åŒ–ä¸ºRAGä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼Œä¼˜åŒ–å¤§æ¨¡å‹ç†è§£"""
        context_parts = []

        # éå†æ£€ç´¢å‡ºçš„æ¯ä¸€ä¸ªå—
        for idx, chunk in enumerate(retrieval_results):
            # 1. æå–å…³é”®ä¿¡æ¯
            vector_score = chunk.get('vector_score', 0)
            bm25_score = chunk.get('bm25_score', 0)
            final_score = chunk.get('final_score', 0)
            file_name = chunk.get('file_origin', 'æœªçŸ¥æ–‡ä»¶')
            page_range = chunk.get('page_range', [])
            text_content = chunk.get('text', '')

            # 2. æ ¼å¼åŒ–é¡µç ä¿¡æ¯ (ä¾‹å¦‚ï¼šP34-35)
            page_info = f"P{page_range[0]}" if page_range else "æœªçŸ¥é¡µç "
            if len(page_range) > 1:
                page_info += f"-{page_range[-1]}"

            # 3. æ„å»ºæ¯ä¸ªå—çš„å±•ç¤ºæ–‡æœ¬
            # ä½¿ç”¨ >>> ç¬¦å·ä½œä¸ºè§†è§‰åˆ†éš”ç¬¦ï¼Œå¸®åŠ©æ¨¡å‹åŒºåˆ†ä¸åŒå¼•ç”¨å—
            chunk_text = f"""
    [å‚è€ƒæ–‡æ¡£ {idx + 1}] (å‘é‡åˆ†æ•°: {vector_score})(bm25åˆ†æ•°: {bm25_score})(åŠ æƒåˆ†æ•°: {final_score})
    ğŸ“‚ æ¥æºæ–‡ä»¶: {file_name}
    ğŸ“„ é¡µç : {page_info}
    ---------------
    {text_content}
    """
            context_parts.append(chunk_text)

        # 4. æ‹¼æ¥æ‰€æœ‰å—ï¼Œä½œä¸ºæ•´ä½“ä¸Šä¸‹æ–‡
        rag_text = "\n".join(context_parts)
        return rag_text

    def hybrid_retriever_chunks(
            self,
            question:str,
            llm_reranking_sample_size:int=8,
            rerank_batch_size:int=4,
            top_n:int=8,
            llm_weight:float=0.6,
    ) -> List[Dict]:
        """
        ä½¿ç”¨æ··åˆæ£€ç´¢æ–¹æ³•è¿›è¡Œæ£€ç´¢å’Œé‡æ’
        :param question: æ£€ç´¢çš„æŸ¥è¯¢è¯­å¥
        :param llm_reranking_sample_size: é¦–è½®å‘é‡æ£€ç´¢è¿”å›çš„å€™é€‰æ•°é‡

        :param top_n: æœ€ç»ˆè¿”å›çš„é‡æ’ç»“æœæ•°é‡
        :param llm_weight: LLMåˆ†æ•°æƒé‡
        :return: ç»è¿‡é‡æ’åçš„æ–‡æ¡£å­—å…¸åˆ—è¡¨ï¼ŒåŒ…å«åˆ†æ•°
        """
        t0 = time.time()
        print(f"[é˜¶æ®µ 1/3] æ··åˆæ£€ç´¢ä¸­...")
        vector_results = self.vector_retriever.get_relevant_chunks(question,top_n=llm_reranking_sample_size)
        print(f"  -> å‘é‡æ£€ç´¢åˆ° {len(vector_results)} ä¸ªç‰‡æ®µ")
        bm25_results = self.bm25_retriever.retrieve(question,top_n=llm_reranking_sample_size)
        print(f"  -> BM25æ£€ç´¢åˆ° {len(bm25_results)} ä¸ªç‰‡æ®µ")
        x = llm_weight # (å‘é‡æ£€ç´¢çš„å æ¯”)
        hybrid_results = self._merge_hybrid_results(vector_results,bm25_results,x)
        hybrid_results_format=self.__format_retrieval_results(hybrid_results)
        # print(hybrid_results_format)
        print(f"  -> èåˆåˆå¹¶å‡º {len(hybrid_results)} ä¸ªç›¸å…³ç‰‡æ®µ")
        t1 = time.time()
        print(f'[HybridRetriever] æ··åˆæ£€ç´¢å®Œæˆï¼Œã€è€—æ—¶ï¼š {t1-t0:.2f} ç§’ã€‘')

        t2 = time.time()
        print(f"\n[é˜¶æ®µ 2/3] LLM é‡æ’ä¸­...")
        print(f"  -> å‡†å¤‡å¯¹ {len(hybrid_results)} ä¸ªå€™é€‰å—è¿›è¡Œé‡æ’ï¼Œæ‰¹æ¬¡å¤§å°: {rerank_batch_size}")
        reranked_results=self.reranker.rerank_chunks(
            question=question,
            retrieved_chunks=hybrid_results,
            top_n=top_n,
            rerank_batch_size=rerank_batch_size,
        )
        t3 = time.time()
        print(f"  -> é‡æ’å®Œæˆï¼Œæœ€ç»ˆé€‰å– Top {len(reranked_results)} ä¸ªå—")
        print(f'[rerank] LLMé‡æ’å®Œæˆï¼Œã€è€—æ—¶ï¼š {t3 - t2:.2f} ç§’ã€‘')
        return reranked_results


