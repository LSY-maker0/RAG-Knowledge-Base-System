"""
main - fastapi å…¥å£æ–‡ä»¶

Author: lsy
Date: 2026/1/22
"""
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import json
import asyncio
import time

from src.api_requests import APIProcessor
from src.retrieval import HybridRetriever,BM25Retriever,VectorRetriever
from src.reranking import LLMReranker
from pathlib import Path

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è®¸æ‰€æœ‰æ¥æºï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®å†™æ­»å…·ä½“å‰ç«¯åœ°å€ï¼Œå¦‚ ["http://localhost:5173"]ï¼‰
    allow_credentials=True,
    allow_methods=["*"],  # å…è®¸æ‰€æœ‰ HTTP æ–¹æ³•
    allow_headers=["*"],  # å…è®¸æ‰€æœ‰è¯·æ±‚å¤´
)

vector_index_path = Path('data/stock_data/databases/vector_dbs/all_reports.faiss')
metadata_path = Path('data/stock_data/databases/vector_dbs/all_metadata.json')

# 1. å®šä¹‰è¯·æ±‚ä½“çš„æ•°æ®æ¨¡å‹
class QuestionRequest(BaseModel):
    question: str

async def search_vector(question):
    vector_retriever = VectorRetriever(vector_index_path, metadata_path)
    vector_results = vector_retriever.get_relevant_chunks(question, top_n=20)
    return vector_results

async def search_bm25(question):
    bm25_retriever = BM25Retriever(metadata_path)
    bm25_results = bm25_retriever.retrieve(question, top_n=20)
    return bm25_results

def hybrid_chunks(vector_results,bm25_results):
    hybrid_retriever = HybridRetriever(vector_index_path=vector_index_path,metadata_path=metadata_path)
    hybrid_results = hybrid_retriever._merge_hybrid_results(vector_results, bm25_results, 0.6)
    return hybrid_results

def rerank_chunks(question,hybrid_results,top_n=8,rerank_batch_size=4):
    reranker = LLMReranker()
    reranked_results = reranker.rerank_chunks(
        question=question,
        retrieved_chunks=hybrid_results,
        top_n=top_n,
        rerank_batch_size=rerank_batch_size,
    )
    return reranked_results

def format_retrieval_results(retrieval_results) -> str:
    """å°†æ£€ç´¢ç»“æœè½¬åŒ–ä¸ºRAGä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼Œä¼˜åŒ–å¤§æ¨¡å‹ç†è§£"""
    context_parts = []

    # éå†æ£€ç´¢å‡ºçš„æ¯ä¸€ä¸ªå—
    for idx, chunk in enumerate(retrieval_results):
        # 1. æå–å…³é”®ä¿¡æ¯
        vector_score = chunk.get('vector_score', 0)
        bm25_score = chunk.get('bm25_score', 0)
        final_score = chunk.get('final_score', 0)
        file_name = chunk.get('file_origin', 'æœªçŸ¥æ–‡ä»¶')
        page_range = chunk.get('page_range', [])
        text_content = chunk.get('text', '')

        # 2. æ ¼å¼åŒ–é¡µç ä¿¡æ¯ (ä¾‹å¦‚ï¼šP34-35)
        page_info = f"P{page_range[0]}" if page_range else "æœªçŸ¥é¡µç "
        if len(page_range) > 1:
            page_info += f"-{page_range[-1]}"

        # 3. æ„å»ºæ¯ä¸ªå—çš„å±•ç¤ºæ–‡æœ¬
        # ä½¿ç”¨ >>> ç¬¦å·ä½œä¸ºè§†è§‰åˆ†éš”ç¬¦ï¼Œå¸®åŠ©æ¨¡å‹åŒºåˆ†ä¸åŒå¼•ç”¨å—
        chunk_text = f"""
[å‚è€ƒæ–‡æ¡£ {idx + 1}] (å‘é‡åˆ†æ•°: {vector_score})(bm25åˆ†æ•°: {bm25_score})(åŠ æƒåˆ†æ•°: {final_score})
ğŸ“‚ æ¥æºæ–‡ä»¶: {file_name}
ğŸ“„ é¡µç : {page_info}
---------------
{text_content}
"""
        context_parts.append(chunk_text)

    # 4. æ‹¼æ¥æ‰€æœ‰å—ï¼Œä½œä¸ºæ•´ä½“ä¸Šä¸‹æ–‡
    rag_text = "\n".join(context_parts)
    return rag_text


# 2. æ¨¡æ‹Ÿä¸€ä¸ªæµå¼ç”Ÿæˆæ•°æ®çš„å‡½æ•° (ä½ å¯ä»¥æŠŠè¿™é‡Œæ›¿æ¢æˆçœŸå®çš„ LLM è°ƒç”¨)
async def generate_rag_response(question: str):
    """
    é€‚é… RAGInterface.vue çš„åç«¯æµå¼ç”Ÿæˆå‡½æ•°
    """

    t0 = time.time()
    # --- æ­¥éª¤ 1: æ¥æ”¶é—®é¢˜ ---
    yield {
        "type": "input",
        "content": {
            "type": "input",
            "title": "ğŸ“¥ æ¥æ”¶é—®é¢˜",
            "data": f"æ”¶åˆ°ç”¨æˆ·é—®é¢˜: {question}",
        }
    }

    # --- æ­¥éª¤ 2: æ£€ç´¢ ---
    data = []
    description = []

    # åˆå§‹æ˜¾ç¤º
    yield {
        "type": "retrieval",
        "content": {
            "type": "retrieval",
            "title": "ğŸ” æ£€ç´¢é˜¶æ®µ",
            "data": data,
            "description": description,
        }
    }
    t1 = time.time()
    vector_results = await search_vector(question=question)
    t2 = time.time()
    description.append('âœ… å‘é‡æ£€ç´¢å®Œæˆ')
    data.append(vector_results)
    # æ›´æ–°åŒä¸€ä¸ªå¡ç‰‡
    yield {
        "type": "retrieval",
        "content": {
            "type": "retrieval",
            "title": "ğŸ” æ£€ç´¢é˜¶æ®µ",
            "data": data,
            "description": description,
            "time": f"è€—æ—¶ {t2-t1:.2f} s"
        }
    }
    t3 = time.time()
    bm25_results = await search_bm25(question=question)
    t4 = time.time()
    description.append('âœ… BM25å…³é”®è¯æ£€ç´¢å®Œæˆ')
    data.append(bm25_results)
    # æ›´æ–°åŒä¸€ä¸ªå¡ç‰‡
    yield {
        "type": "retrieval",
        "content": {
            "type": "retrieval",
            "title": "ğŸ” æ£€ç´¢é˜¶æ®µ",
            "data": data,
            "description": description,
            "time": f"è€—æ—¶ {t4-t3+t2-t1:.2f} s"
        }
    }

    t5 = time.time()
    hybrid_results = hybrid_chunks(vector_results, bm25_results)
    t6 = time.time()
    description.append('âœ… æ··åˆåˆå¹¶å®Œæˆ')
    data.append(hybrid_results)
    # æ›´æ–°åŒä¸€ä¸ªå¡ç‰‡
    yield {
        "type": "retrieval",
        "content": {
            "type": "retrieval",
            "title": "ğŸ” æ£€ç´¢é˜¶æ®µ",
            "data": data,
            "description": description,
            "time": f"è€—æ—¶ {t6-t5+t4-t3+t2-t1:.2f} s"
        }
    }

    t7 = time.time()
    rerank_results = rerank_chunks(question=question,hybrid_results=hybrid_results,top_n=8,rerank_batch_size=4)
    t8 = time.time()

    # --- å‘é€å‚è€ƒæ–‡æ¡£ ---
    yield {
        "type": "rerank",
        "content": {
            "type": "rerank",
            "title": 'ğŸ§  LLMé‡æ’é˜¶æ®µ',
            "description": 'âœ… LLM é‡æ’å®Œæˆ',
            "data": rerank_results,
            "time": f"è€—æ—¶ {t8-t7:.2f} s"
        }
    }

    # --- æ­¥éª¤ 3: ç”Ÿæˆç­”æ¡ˆ (æ‰“å­—æœºæ•ˆæœ) ---
    api_processor = APIProcessor()
    rag_context = format_retrieval_results(rerank_results)
    
    # è·å–çœŸå®çš„LLMæµå¼å“åº”
    responses = api_processor.get_answer_from_rag_context(
        question=question,
        rag_context=rag_context,
        kind="summary",  # æ·»åŠ ç¼ºå¤±çš„å‚æ•°
        model='qwen-turbo-latest',  # ä½¿ç”¨å®é™…æ¨¡å‹åç§°è€Œä¸æ˜¯'dashscope'
        stream=True  # å¯ç”¨æµå¼è¾“å‡º
    )
    
    # å¤„ç†æµå¼å“åº”
    full_answer = ""
    for response in responses:
        if hasattr(response, 'output') and hasattr(response.output, 'choices'):
            content = response.output.choices[0].message.content or ""
        else:
            content = ""
        
        # å°†æ¯ä¸ªå“åº”å†…å®¹é€å­—ç¬¦å‘é€
        for char in content:
            full_answer += char
            yield {
                "type": "answer",
                "data": char
            }

    t9 = time.time()
    print("æœ€ç»ˆç­”æ¡ˆ:", full_answer)

    # --- ç»“æŸ ---
    yield {
        "type": "done",
        "timing": f"æ€»è€—æ—¶ {t9 - t0:.2f} s"
    }


# è¾…åŠ©å‡½æ•°ï¼šå°†å­—å…¸è½¬æ¢ä¸º SSE æ ¼å¼ (data: {...}\n\n)
async def event_generator(question: str):
    """ç”Ÿæˆ SSE æ ¼å¼çš„æµæ•°æ®"""
    async for chunk in generate_rag_response(question):
        json_str = json.dumps(chunk, ensure_ascii=False)
        # æ ‡å‡† SSE æ ¼å¼
        yield f"data: {json_str}\n\n"


# 3. å®šä¹‰æ¥å£
@app.post("/query")
async def chat_endpoint(request: QuestionRequest):
    """
    æµå¼èŠå¤©æ¥å£ - è¾¹ç”Ÿæˆè¾¹ä¼ è¾“
    """
    return StreamingResponse(
        event_generator(request.question),  # ä¼ å…¥ç”¨æˆ·çš„é—®é¢˜
        media_type="text/event-stream"  # æŒ‡å®šåª’ä½“ç±»å‹ä¸º SSE
    )


# è¿è¡ŒæœåŠ¡å™¨
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)
